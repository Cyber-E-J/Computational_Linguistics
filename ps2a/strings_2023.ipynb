{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentences about strings\n",
    "Sentences about strings are a demo domain for semantic grammars, where sentences are true\n",
    "or false as descriptions of strings.  \n",
    "## Base grammar including *follows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import grammar, parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# Grammar Rules\n",
      "S[SEM=<?subj(?vp)>] -> DP[SEM=?subj] VP[SEM=?vp]\n",
      "S[SEM=<?vp(?subj)>] -> NP[SEM=?subj] VP[SEM=?vp]\n",
      "DP[ SEM=<?X(?P)>] -> Det[SEM=?X] N[SEM=?P] \n",
      "VP[SEM=?Q] -> 'is' A[SEM=?Q]\n",
      "# This is included for testing.\n",
      "VP[SEM=<\\x.offend(x)>] -> 'offends'\n",
      "# Transitive verb with individual object.\n",
      "VP[ SEM=<?R(?n)>] -> TV[SEM=?R] NP[SEM=?n]\n",
      "# Transitive verb with quantifier object.\n",
      "# The object is given minimal scope.\n",
      "VP[ SEM=<\\m.?X(\\n.(?R(n)(m)))>] -> TV[SEM=?R] DP[SEM=?X]\n",
      "# Lexical Rules\n",
      "A[SEM=<\\n.exists c.(vowel(c) & char(n,c))>] -> 'vocalic'\n",
      "A[SEM=<\\n.exists c.((-vowel(c)) & char(n,c))>] -> 'consonantal'\n",
      "Det[SEM=<\\P Q.all n.(P(n) -> Q(n))>] -> 'every'\n",
      "Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'a'\n",
      "Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'some'\n",
      "N[SEM=<\\n.char(n,leti)>] -> 'i'\n",
      "N[SEM=<\\n.char(n,lete)>] -> 'e'\n",
      "N[SEM=<\\n.char(n,letu)>] -> 'u'\n",
      "N[SEM=<\\n.char(n,letp)>] -> 'p'\n",
      "N[SEM=<\\n.char(n,lett)>] -> 't'\n",
      "N[SEM=<\\n.char(n,letk)>] -> 'k'\n",
      "N[SEM=<\\n.exists c.char(n,c)>] -> 'letter'\n",
      "N[SEM=<\\n.exists c.(vowel(c) & char(n,c))>] -> 'vowel'\n",
      "NP[SEM=<1>] -> 'letter' 'one'\n",
      "NP[SEM=<2>] -> 'letter' 'two'\n",
      "NP[SEM=<3>] -> 'letter' 'three'\n",
      "NP[SEM=<4>] -> 'letter' 'four'\n",
      "NP[SEM=<5>] -> 'letter' 'five'\n",
      "TV[SEM=<\\m n.le(m,n)>] -> 'follows'\n",
      "TV[SEM=<\\m n.le(n,m)>] -> 'precedes'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('h2.fcfg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function parses a whitespace separated sentence `s` using a parser `p`, and displays\n",
    "the trees graphically or as labeled bracketings. Specify`graphic=False` if graphics don't\n",
    "work in your notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_display(s, p, graphic:bool=True):\n",
    "   for tree in p.parse(s.split()): \n",
    "    if graphic: \n",
    "        tree.draw()\n",
    "    else: print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimal example with *follows* and token characters as subject and object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.parse.util import load_parser\n",
    "pr = load_parser('h2.fcfg', trace=0,cache=False)\n",
    "parse_and_display('letter three follows letter one',pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[SEM=<le(3,1)>]\n",
      "  (NP[SEM=<1>] letter one)\n",
      "  (VP[SEM=<\\n.le(3,n)>]\n",
      "    (TV[SEM=<\\m n.le(m,n)>] follows)\n",
      "    (NP[SEM=<3>] letter three)))\n"
     ]
    }
   ],
   "source": [
    "parse_and_display('letter one follows letter three',pr,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Minimal two-word sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_and_display('letter one offends',pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[SEM=<exists n.(char(n,lett) & le(1,n))>]\n",
      "  (DP[SEM=<\\Q.exists n.(char(n,lett) & Q(n))>]\n",
      "    (Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] a)\n",
      "    (N[SEM=<\\n.char(n,lett)>] t))\n",
      "  (VP[SEM=<\\n.le(1,n)>]\n",
      "    (TV[SEM=<\\m n.le(m,n)>] follows)\n",
      "    (NP[SEM=<1>] letter one)))\n"
     ]
    }
   ],
   "source": [
    "parse_and_display('a t follows letter one',pr,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Idiom for naming a parse tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DP (quantified nominal) in object position\n",
    "This production fits the DP semantics in using a dummy variable\n",
    "*m* in the subject position.  It scopes the DP over just the verb.\n",
    "```\n",
    "VP[ SEM=<\\m.?X(\\n.(?R(n)(m)))>] -> TV[SEM=?R] DP[SEM=?X]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_and_display('letter four follows every u',pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Truth checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = \"\"\"\n",
    "1 => 1\n",
    "2 => 2\n",
    "3 => 3\n",
    "4 => 4\n",
    "5 => 5\n",
    "leti => i\n",
    "lete => e\n",
    "letu => u\n",
    "letp => p\n",
    "lett => t\n",
    "letk => k\n",
    "vowel => {i, e, u}\n",
    "char => {(1,t), (2,u), (3, t),(4,u),(5,k)}\n",
    "le => {(1,2),(1,3),(1,4),(1,5),(2,3),(2,4),(2,5),(3,4),(3,5),(4,5)}\n",
    "\"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain = {'3', 't', 'u', '2', 'p', 'e', '5', '4', 'i', '1', 'k'},\n",
      "Valuation = \n",
      "{'1': '1',\n",
      " '2': '2',\n",
      " '3': '3',\n",
      " '4': '4',\n",
      " '5': '5',\n",
      " 'char': {('3', 't'), ('5', 'k'), ('1', 't'), ('4', 'u'), ('2', 'u')},\n",
      " 'le': {('1', '2'),\n",
      "        ('1', '3'),\n",
      "        ('1', '4'),\n",
      "        ('1', '5'),\n",
      "        ('2', '3'),\n",
      "        ('2', '4'),\n",
      "        ('2', '5'),\n",
      "        ('3', '4'),\n",
      "        ('3', '5'),\n",
      "        ('4', '5')},\n",
      " 'lete': 'e',\n",
      " 'leti': 'i',\n",
      " 'letk': 'k',\n",
      " 'letp': 'p',\n",
      " 'lett': 't',\n",
      " 'letu': 'u',\n",
      " 'vowel': {('i',), ('e',), ('u',)}}\n"
     ]
    }
   ],
   "source": [
    "val = nltk.Valuation.fromstring(v)\n",
    "model_tutuk = nltk.Model(val.domain, val)\n",
    "print(model_tutuk)\n",
    "ass = nltk.Assignment(val.domain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le(1,3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1 = 'letter three follows letter one'.split()\n",
    "f1=next(pr.parse(s1)).label()['SEM']\n",
    "print(f1)\n",
    "model_tutuk.evaluate(str(f1),ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all n.(char(n,letu) -> le(1,n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = 'every u follows letter one'.split()\n",
    "f3=next(pr.parse(s3)).label()['SEM']\n",
    "print(f3)\n",
    "model_tutuk.evaluate(str(f3),ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all n.(char(n,lett) -> le(1,n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = 'every t follows letter one'.split()\n",
    "f4=next(pr.parse(s4)).label()['SEM']\n",
    "print(f4)\n",
    "model_tutuk.evaluate(str(f4),ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all n.(char(n,letu) -> le(1,n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s3 = 'every u follows letter one'.split()\n",
    "f3=next(pr.parse(s3)).label()['SEM']\n",
    "print(f3)\n",
    "model_tutuk.evaluate(str(f3),ass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all n.(char(n,lett) -> le(1,n))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s4 = 'every t follows letter one'.split()\n",
    "f4=next(pr.parse(s4)).label()['SEM']\n",
    "print(f4)\n",
    "model_tutuk.evaluate(str(f4),ass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model creation\n",
    "This is Angela Liu's code for flexibly mapping a string to a string form of a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List, Set\n",
    "\n",
    "def to_model_str(word: str, special_rels: List[Callable[[str], str]]=[]) -> str:\n",
    "    \"\"\"\n",
    "    Creates the string form of the model for the input word. This string is meant to be passed to `nltk.Valuation.fromstring`.\n",
    "    By default, the function will only add the relations mapping i => i for i from 1 to the length of `word` and a relation \n",
    "    mapping char => the set of tuples (i, word[i]). The `special_rels` function allows you to specify additional relations to \n",
    "    be added to the valuation string.\n",
    "    \n",
    "    :param word: The word to create a model string for.\n",
    "    :param special_rels: A list of functions that when called return a string of the form {relation_name} => {relation_contents}. Defaults to the empty list.\n",
    "    :returns: a string representing the model for word\n",
    "    \"\"\"\n",
    "    n = len(word)\n",
    "    model_str = []\n",
    "    char = []\n",
    "    for i in range(1, n+1):\n",
    "        model_str.append(f'{i} => {i}')\n",
    "        char.append((i, word[i-1]))\n",
    "    model_str.append(f'char => {set(char)}'.lower())\n",
    "    return '\\n'.join(model_str + [rel(word) for rel in special_rels]).replace(\"'\", \"\")\n",
    "# Angela Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The argument `special_rels` is used to introduce relation symbols such as *le*, *vowel*, or *capital*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "get_vowel = lambda w: f'vowel => {set(re.findall(r\"[AEIOUaeiou]\", w))}'.lower()\n",
    "get_cons = lambda w: 'cons => {}'.format(set(re.findall(r\"[^AEIOUaeiou\\W0-9]\", w))).lower()\n",
    "follows = lambda w: f'le => {set([(i+1,j+1) for i in range(len(w)) for j in range(i, len(w)) if i != j])}'\n",
    "get_capital = lambda w: f'capital => {set([m.span()[0] + 1 for m in re.finditer(r\"[A-Z]\", w)])}'\n",
    "# Angela Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function fixes a problems with the representation of the empty set in valuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def emptysets(val:nltk.sem.evaluate.Valuation):\n",
    "  val.update([(k,set()) for (k,v) in val.items() if v == 'set()'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example of model construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "----------------\n",
      "Domain = {'3', 't', 'c', 'a', '2', '1'},\n",
      "Valuation = \n",
      "{'1': '1',\n",
      " '2': '2',\n",
      " '3': '3',\n",
      " 'capital': set(),\n",
      " 'char': {('2', 'a'), ('3', 't'), ('1', 'c')},\n",
      " 'cons': {('t',), ('c',)},\n",
      " 'le': {('1', '2'), ('2', '3'), ('1', '3')},\n",
      " 'vowel': {('a',)}}\n",
      "\n",
      "mAtch\n",
      "----------------\n",
      "Domain = {'3', 't', 'h', 'c', 'a', '2', 'm', '5', '4', '1'},\n",
      "Valuation = \n",
      "{'1': '1',\n",
      " '2': '2',\n",
      " '3': '3',\n",
      " '4': '4',\n",
      " '5': '5',\n",
      " 'capital': {('2',)},\n",
      " 'char': {('3', 't'), ('5', 'h'), ('4', 'c'), ('2', 'a'), ('1', 'm')},\n",
      " 'cons': {('t',), ('m',), ('h',), ('c',)},\n",
      " 'le': {('1', '2'),\n",
      "        ('1', '3'),\n",
      "        ('1', '4'),\n",
      "        ('1', '5'),\n",
      "        ('2', '3'),\n",
      "        ('2', '4'),\n",
      "        ('2', '5'),\n",
      "        ('3', '4'),\n",
      "        ('3', '5'),\n",
      "        ('4', '5')},\n",
      " 'vowel': {('a',)}}\n",
      "\n",
      "peRiLOuS\n",
      "----------------\n",
      "Domain = {'s', 'l', '7', '3', '8', 'u', '6', '2', 'o', 'r', 'p', 'e', '5', '4', 'i', '1'},\n",
      "Valuation = \n",
      "{'1': '1',\n",
      " '2': '2',\n",
      " '3': '3',\n",
      " '4': '4',\n",
      " '5': '5',\n",
      " '6': '6',\n",
      " '7': '7',\n",
      " '8': '8',\n",
      " 'capital': {('6',), ('3',), ('5',), ('8',)},\n",
      " 'char': {('1', 'p'),\n",
      "          ('2', 'e'),\n",
      "          ('3', 'r'),\n",
      "          ('4', 'i'),\n",
      "          ('5', 'l'),\n",
      "          ('6', 'o'),\n",
      "          ('7', 'u'),\n",
      "          ('8', 's')},\n",
      " 'cons': {('r',), ('l',), ('p',), ('s',)},\n",
      " 'le': {('1', '2'),\n",
      "        ('1', '3'),\n",
      "        ('1', '4'),\n",
      "        ('1', '5'),\n",
      "        ('1', '6'),\n",
      "        ('1', '7'),\n",
      "        ('1', '8'),\n",
      "        ('2', '3'),\n",
      "        ('2', '4'),\n",
      "        ('2', '5'),\n",
      "        ('2', '6'),\n",
      "        ('2', '7'),\n",
      "        ('2', '8'),\n",
      "        ('3', '4'),\n",
      "        ('3', '5'),\n",
      "        ('3', '6'),\n",
      "        ('3', '7'),\n",
      "        ('3', '8'),\n",
      "        ('4', '5'),\n",
      "        ('4', '6'),\n",
      "        ('4', '7'),\n",
      "        ('4', '8'),\n",
      "        ('5', '6'),\n",
      "        ('5', '7'),\n",
      "        ('5', '8'),\n",
      "        ('6', '7'),\n",
      "        ('6', '8'),\n",
      "        ('7', '8')},\n",
      " 'vowel': {('i',), ('e',), ('u',), ('o',)}}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "words = ['cat', 'mAtch', 'peRiLOuS']\n",
    "vals = [nltk.Valuation.fromstring(to_model_str(w, [get_vowel, get_cons, follows, get_capital])) for w in words]\n",
    "for v in vals: emptysets(v)\n",
    "models = [nltk.Model(val.domain, val) for val in vals]\n",
    "for w, m in zip(words, models):\n",
    "    print(f'{w}\\n----------------\\n{m}\\n')\n",
    "# Angela Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example solutions for ps2\n",
    "The problems in problem set 2 ask us to define the syntax and semantics of\n",
    "particular words and/or constructions.  This normally entails adding one\n",
    "or more lexical productions to the grammar, but also could involve adding\n",
    "non-lexical productions.\n",
    "\n",
    "### Example problem 1\n",
    "Define the syntax and semantics of *non-* as used in this sentence.\n",
    "Treat it as a separate word.\n",
    "\n",
    "*letter two is non- vocalic*\n",
    "\n",
    "### Example solution 1\n",
    "The solution is given as a sequence of cells, some of which are Markup cells,\n",
    "and others code cells.  The markup cells should explain the strategy and \n",
    "comment on the examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are my example words, and the intuitive truth values of sentence s1 as a description\n",
    "of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "e0 = [('emu',True),('bat',False)]\n",
    "s0 = 'letter two is non- vocalic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined *non-* as an Adv (adverb), using these productions. The Adv is semantically the function.\n",
    "The adverb `non-` semantically takes the complement of the input adjective.\n",
    "```\n",
    "A[SEM=<?f(?P)>] -> Adv[SEM=?f] A[SEM=?P]\n",
    "ADV[SEM=<\\P x.-P(x)>] -> 'non-'\n",
    "```\n",
    "\n",
    "This is the tree for the target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mlocal\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('local')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93m/local/teach/cl21/grammar/h3b0.fcfg\u001b[0m\n\n  Searched in:\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/n1/c0nqpbx13p13jsk3pkzp36yc0000gn/T/ipykernel_46865/72767715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpr1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/local/teach/cl21/grammar/h3b0.fcfg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparse_and_display\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpr1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CompLing/lib/python3.7/site-packages/nltk/parse/util.py\u001b[0m in \u001b[0;36mload_parser\u001b[0;34m(grammar_url, trace, parser, chart_class, beam_size, **load_args)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minformation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \"\"\"\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0mgrammar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mload_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The grammar must be a CFG, \"\u001b[0m \u001b[0;34m\"or a subclass thereof.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CompLing/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[1;32m    750\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m     \u001b[0;31m# Load the resource.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m     \u001b[0mopened_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CompLing/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(resource_url)\u001b[0m\n\u001b[1;32m    878\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0;31m# urllib might not use mode='rb', so handle this one ourselves:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 880\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    881\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    882\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/CompLing/lib/python3.7/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mlocal\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('local')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93m/local/teach/cl21/grammar/h3b0.fcfg\u001b[0m\n\n  Searched in:\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "pr1 = load_parser('/local/teach/cl21/grammar/h3b0.fcfg', trace=0,cache=False)\n",
    "parse_and_display(s0,pr1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 27 productions (start state = S[])\n",
      "    S[SEM=<?subj(?vp)>] -> DP[SEM=?subj] VP[SEM=?vp]\n",
      "    S[SEM=<?vp(?subj)>] -> NP[SEM=?subj] VP[SEM=?vp]\n",
      "    DP[SEM=<?X(?P)>] -> Det[SEM=?X] N[SEM=?P]\n",
      "    VP[SEM=?Q] -> 'is' A[SEM=?Q]\n",
      "    VP[SEM=<\\x.offend(x)>] -> 'offends'\n",
      "    VP[SEM=<?R(?n)>] -> TV[SEM=?R] NP[SEM=?n]\n",
      "    VP[SEM=<\\m.?X(\\n.?R(n,m))>] -> TV[SEM=?R] DP[SEM=?X]\n",
      "    A[SEM=<?f(?P)>] -> Adv[SEM=?f] A[SEM=?P]\n",
      "    A[SEM=<\\n.exists c.(vowel(c) & char(n,c))>] -> 'vocalic'\n",
      "    Det[SEM=<\\P Q.all n.(P(n) -> Q(n))>] -> 'every'\n",
      "    Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'a'\n",
      "    Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'some'\n",
      "    Det[SEM=<\\P Q.exists n.(P(n) & Q(n) & all m.((P(m) & Q(m)) -> eq(m,n)))>] -> 'exactly' 'one'\n",
      "    N[SEM=<\\n.char(n,leti)>] -> 'i'\n",
      "    N[SEM=<\\n.char(n,lete)>] -> 'e'\n",
      "    N[SEM=<\\n.char(n,letu)>] -> 'u'\n",
      "    N[SEM=<\\n.char(n,letp)>] -> 'p'\n",
      "    N[SEM=<\\n.char(n,lett)>] -> 't'\n",
      "    N[SEM=<\\n.char(n,letk)>] -> 'k'\n",
      "    N[SEM=<\\n.exists c.char(n,c)>] -> 'letter'\n",
      "    NP[SEM=<1>] -> 'letter' 'one'\n",
      "    NP[SEM=<2>] -> 'letter' 'two'\n",
      "    NP[SEM=<3>] -> 'letter' 'three'\n",
      "    NP[SEM=<4>] -> 'letter' 'four'\n",
      "    NP[SEM=<5>] -> 'letter' 'five'\n",
      "    TV[SEM=<\\m n.le(m,n)>] -> 'follows'\n",
      "    Adv[SEM=<\\P x.-P(x)>] -> 'non-'\n"
     ]
    }
   ],
   "source": [
    "print(pr1.grammar())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-exists c.(vowel(c) & char(2,c))\n"
     ]
    }
   ],
   "source": [
    "pr0 = load_parser('/local/teach/cl21/grammar/h3b0.fcfg', trace=0,cache=False)\n",
    "f0 = next(pr0.parse(s0.split())).label()['SEM']\n",
    "print(f0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This constructs the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [e[0] for e in e0]\n",
    "truths = [e[1] for e in e0]\n",
    "vals = [nltk.Valuation.fromstring(to_model_str(w, [get_vowel, follows])) for w in words]\n",
    "assignments = [nltk.Assignment(val.domain) for val in vals]\n",
    "for val in vals: emptysets(val)\n",
    "models = [nltk.Model(val.domain, val) for val in vals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluates formula `f0` in the three models.  The results are as desired.  The\n",
    "second and third models have a non-vocalic character in position two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letter two is non- vocalic\n",
      "---------------\n",
      "emu\n",
      "True\n",
      "----------------\n",
      "bat\n",
      "False\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "print(f'{s0}\\n---------------')\n",
    "for w, a, m in zip(words, assignments, models):\n",
    "    print(f'{w}\\n{m.evaluate(str(f0),a)}\\n----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example problem 2\n",
    "Define the syntax and semantics of *exactly one* as used in this sentence.\n",
    "\n",
    "*exactly one letter is vocalic*\n",
    "\n",
    "### Example solution 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are my example words, and the intuitive truth values of sentence s1 as a description\n",
    "of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e1 = [('au',False),('at',True),('pt',False)]\n",
    "s1 = 'exactly one letter is vocalic'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This defines the formula labeling root of the parse tree for `s1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exists n.(exists c.char(n,c) & exists c.(vowel(c) & char(n,c)) & all m.((exists c.char(m,c) & exists c.(vowel(c) & char(m,c))) -> eq(m,n)))\n"
     ]
    }
   ],
   "source": [
    "pr1 = load_parser('/local/teach/cl21/grammar/h3b0.fcfg', trace=0,cache=False)\n",
    "f1 = next(pr0.parse(s1.split())).label()['SEM']\n",
    "print(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This constructs the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [e[0] for e in e1]\n",
    "truths = [e[1] for e in e1]\n",
    "equals = lambda w: f'eq => {set([(i+1,i+1) for i in range(len(w))])}'\n",
    "vals = [nltk.Valuation.fromstring(to_model_str(w, [get_vowel, follows, equals])) for w in words]\n",
    "assignments = [nltk.Assignment(val.domain) for val in vals]\n",
    "for val in vals: emptysets(val)\n",
    "models = [nltk.Model(val.domain, val) for val in vals]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evaluates formula `f1` in the three models.  The results are as desired.  Only the middle word\n",
    "has exactly one vowel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exactly one letter is vocalic\n",
      "---------------\n",
      "au\n",
      "False\n",
      "----------------\n",
      "at\n",
      "True\n",
      "----------------\n",
      "pt\n",
      "False\n",
      "----------------\n"
     ]
    }
   ],
   "source": [
    "print(f'{s1}\\n---------------')\n",
    "for w, a, m in zip(words, assignments, models):\n",
    "    print(f'{w}\\n{m.evaluate(str(f1),a)}\\n----------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I defined *exactly one* as a determiner, using a production with this syntax.\n",
    "```\n",
    "Det[] -> 'exactly' 'one'\n",
    "```\n",
    "The idea for that semantics is that there should be some token character that\n",
    "satisfies the N and the VP, but there should not be an additional, different\n",
    "token character which also satifies them. This is expressed using a two-place\n",
    "relation *eq*, which is understood to be the equality relation on numbers.\n",
    "```\n",
    "Det[SEM=<\\P Q.exists n.(P(n) & Q(n) & all m.((P(m) & Q(m)) -> eq(m,n)))>] -> 'exactly' 'one'\n",
    "```\n",
    "This is the tree for the target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_and_display(s1,pr1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = pr.grammar()\n",
    "from nltk.parse.generate import generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grammar with 24 productions (start state = S[])\n",
      "    S[SEM=<?subj(?vp)>] -> DP[SEM=?subj] VP[SEM=?vp]\n",
      "    S[SEM=<?vp(?subj)>] -> NP[SEM=?subj] VP[SEM=?vp]\n",
      "    DP[SEM=<?X(?P)>] -> Det[SEM=?X] N[SEM=?P]\n",
      "    VP[SEM=?Q] -> 'is' A[SEM=?Q]\n",
      "    VP[SEM=<\\x.offend(x)>] -> 'offends'\n",
      "    VP[SEM=<?R(?n)>] -> TV[SEM=?R] NP[SEM=?n]\n",
      "    VP[SEM=<\\m.?X(\\n.?R(n,m))>] -> TV[SEM=?R] DP[SEM=?X]\n",
      "    A[SEM=<\\n.exists c.(vowel(c) & char(n,c))>] -> 'vocalic'\n",
      "    Det[SEM=<\\P Q.all n.(P(n) -> Q(n))>] -> 'every'\n",
      "    Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'a'\n",
      "    Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'some'\n",
      "    N[SEM=<\\n.char(n,leti)>] -> 'i'\n",
      "    N[SEM=<\\n.char(n,lete)>] -> 'e'\n",
      "    N[SEM=<\\n.char(n,letu)>] -> 'u'\n",
      "    N[SEM=<\\n.char(n,letp)>] -> 'p'\n",
      "    N[SEM=<\\n.char(n,lett)>] -> 't'\n",
      "    N[SEM=<\\n.char(n,letk)>] -> 'k'\n",
      "    N[SEM=<\\n.exists c.char(n,c)>] -> 'letter'\n",
      "    NP[SEM=<1>] -> 'letter' 'one'\n",
      "    NP[SEM=<2>] -> 'letter' 'two'\n",
      "    NP[SEM=<3>] -> 'letter' 'three'\n",
      "    NP[SEM=<4>] -> 'letter' 'four'\n",
      "    NP[SEM=<5>] -> 'letter' 'five'\n",
      "    TV[SEM=<\\m n.le(m,n)>] -> 'follows'\n"
     ]
    }
   ],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "every i is vocalic\n",
      "every i offends\n",
      "every i follows letter one\n",
      "every i follows letter two\n",
      "every i follows letter three\n",
      "every i follows letter four\n",
      "every i follows letter five\n",
      "every i follows every i\n",
      "every i follows every e\n",
      "every i follows every u\n",
      "every i follows every p\n",
      "every i follows every t\n",
      "every i follows every k\n",
      "every i follows every letter\n",
      "every i follows a i\n",
      "every i follows a e\n",
      "every i follows a u\n",
      "every i follows a p\n",
      "every i follows a t\n",
      "every i follows a k\n",
      "every i follows a letter\n",
      "every i follows some i\n",
      "every i follows some e\n",
      "every i follows some u\n",
      "every i follows some p\n",
      "every i follows some t\n",
      "every i follows some k\n",
      "every i follows some letter\n",
      "every e is vocalic\n",
      "every e offends\n",
      "every e follows letter one\n",
      "every e follows letter two\n",
      "every e follows letter three\n",
      "every e follows letter four\n",
      "every e follows letter five\n",
      "every e follows every i\n",
      "every e follows every e\n",
      "every e follows every u\n",
      "every e follows every p\n",
      "every e follows every t\n",
      "every e follows every k\n",
      "every e follows every letter\n",
      "every e follows a i\n",
      "every e follows a e\n",
      "every e follows a u\n",
      "every e follows a p\n",
      "every e follows a t\n",
      "every e follows a k\n",
      "every e follows a letter\n",
      "every e follows some i\n",
      "every e follows some e\n",
      "every e follows some u\n",
      "every e follows some p\n",
      "every e follows some t\n",
      "every e follows some k\n",
      "every e follows some letter\n",
      "every u is vocalic\n",
      "every u offends\n",
      "every u follows letter one\n",
      "every u follows letter two\n",
      "every u follows letter three\n",
      "every u follows letter four\n",
      "every u follows letter five\n",
      "every u follows every i\n",
      "every u follows every e\n",
      "every u follows every u\n",
      "every u follows every p\n",
      "every u follows every t\n",
      "every u follows every k\n",
      "every u follows every letter\n",
      "every u follows a i\n",
      "every u follows a e\n",
      "every u follows a u\n",
      "every u follows a p\n",
      "every u follows a t\n",
      "every u follows a k\n",
      "every u follows a letter\n",
      "every u follows some i\n",
      "every u follows some e\n",
      "every u follows some u\n",
      "every u follows some p\n",
      "every u follows some t\n",
      "every u follows some k\n",
      "every u follows some letter\n",
      "every p is vocalic\n",
      "every p offends\n",
      "every p follows letter one\n",
      "every p follows letter two\n",
      "every p follows letter three\n",
      "every p follows letter four\n",
      "every p follows letter five\n",
      "every p follows every i\n",
      "every p follows every e\n",
      "every p follows every u\n",
      "every p follows every p\n",
      "every p follows every t\n",
      "every p follows every k\n",
      "every p follows every letter\n",
      "every p follows a i\n",
      "every p follows a e\n"
     ]
    }
   ],
   "source": [
    "for sentence in generate(g, n=100, depth=6):\n",
    "...     print(' '.join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompLing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "6efd08fb7d61c09f4aaea9e8ceb0bf572fefe064d17a4cedc1b735c081f854a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
