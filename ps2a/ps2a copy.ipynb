{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.7.16'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import grammar, parse\n",
    "from nltk.parse.generate import generate\n",
    "from platform import python_version\n",
    "python_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5\n"
     ]
    }
   ],
   "source": [
    "print(nltk.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% start S\n",
      "# Grammar Rules\n",
      "S[SEM=<?subj(?vp)>] -> DP[SEM=?subj] VP[SEM=?vp]\n",
      "S[SEM=<?vp(?subj)>] -> NP[SEM=?subj] VP[SEM=?vp]\n",
      "DP[ SEM=<?X(?P)>] -> Det[SEM=?X] N[SEM=?P] \n",
      "VP[SEM=?Q] -> 'is' A[SEM=?Q]\n",
      "# This is included for testing.\n",
      "VP[SEM=<\\x.offend(x)>] -> 'offends'\n",
      "# Transitive verb with individual object.\n",
      "VP[ SEM=<?R(?n)>] -> TV[SEM=?R] NP[SEM=?n]\n",
      "# Transitive verb with quantifier object.\n",
      "# The object is given minimal scope.\n",
      "VP[ SEM=<\\m.?X(\\n.(?R(n)(m)))>] -> TV[SEM=?R] DP[SEM=?X]\n",
      "# Lexical Rules\n",
      "A[SEM=<\\n.exists c.(vowel(c) & char(n,c))>] -> 'vocalic'\n",
      "Det[SEM=<\\P Q.all n.(P(n) -> Q(n))>] -> 'every'\n",
      "Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'a'\n",
      "Det[SEM=<\\P Q.exists n.(P(n) & Q(n))>] -> 'some'\n",
      "N[SEM=<\\n.char(n,leti)>] -> 'i'\n",
      "N[SEM=<\\n.char(n,lete)>] -> 'e'\n",
      "N[SEM=<\\n.char(n,letu)>] -> 'u'\n",
      "N[SEM=<\\n.char(n,letp)>] -> 'p'\n",
      "N[SEM=<\\n.char(n,lett)>] -> 't'\n",
      "N[SEM=<\\n.char(n,letk)>] -> 'k'\n",
      "N[SEM=<\\n.exists c.char(n,c)>] -> 'letter'\n",
      "NP[SEM=<1>] -> 'letter' 'one'\n",
      "NP[SEM=<2>] -> 'letter' 'two'\n",
      "NP[SEM=<3>] -> 'letter' 'three'\n",
      "NP[SEM=<4>] -> 'letter' 'four'\n",
      "NP[SEM=<5>] -> 'letter' 'five'\n",
      "TV[SEM=<\\m n.le(m,n)>] -> 'follows'\n"
     ]
    }
   ],
   "source": [
    "nltk.data.show_cfg('h2.fcfg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S[SEM=<exists c.(vowel(c) & char(2,c))>]\n",
      "  (NP[SEM=<2>] letter two)\n",
      "  (VP[SEM=<\\n.exists c.(vowel(c) & char(n,c))>]\n",
      "    is\n",
      "    (A[SEM=<\\n.exists c.(vowel(c) & char(n,c))>] vocalic)))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.load_parser('h2.fcfg', trace=0)\n",
    "sent = 'letter two is vocalic'.split()\n",
    "for tree in cp.parse(sent): print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。请查看单元格中的代码，以确定故障的可能原因。有关详细信息，请单击 <a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>。有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "tree.draw()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Semantics of sentences about strings\n",
    "Computational Linguistics Spring 2023\n",
    "Problems Set 2\n",
    "\n",
    "The text for this module is the NLTK book\n",
    "Chapter 9. Building Feature Based Grammars and\n",
    "Chapter 10. Analyzing the Meaning of Sentences\n",
    "\n",
    "See also lecture8_2023.ipynb and string_2023.ipynb\n",
    "\n",
    "The purpose of the assingnment is to develop feature-based grammars that include\n",
    "logical semantics, and to evaluate the adequacy of the semantics by computing\n",
    "truth in logically constructed models.  For instance, we want to be able to\n",
    "evaluate whether the sentence\n",
    "\n",
    "   every consonant is capitalized\n",
    "\n",
    "is true or false as description of the word\n",
    "\n",
    "   CINEMA\n",
    "\n",
    "or\n",
    "\n",
    "   Cinema\n",
    "\n",
    "or\n",
    "\n",
    "   CINEmA.\n",
    "\n",
    "In each problem n do these steps. The problem statement gives sentence sn. See\n",
    "Chapters 9 and 10 for the methodology.\n",
    "\n",
    "(i) Define a feature based grammar gn that includes all the words in sentence sn\n",
    "its lexicon.  The feature grammars will usually add a word and/or construction\n",
    "to a base grammar which will be similar to simple-sem.fcfg.  This base grammar\n",
    "will be distributed. (It will be helpful to figure out how to add a lexical item\n",
    "or production to a grammar in Python. Discuss the method for this on the forum.\n",
    "Or you can define the grammar from scratch.)\n",
    "\n",
    "(ii) Parse the sentenced and display the tree.\n",
    "\n",
    "(iii) Map sn to a logical formula fn by parsing with gn and extracting the\n",
    "semantics that annotates the root.\n",
    "\n",
    "(iv) Define a combination four words (serving as models) and the intuitive\n",
    "truth values of sentence sn as a description of the word.\n",
    "\n",
    "(v) Transform the four words into four models or valuations in the sense of\n",
    "Chapter 10. This can be done as in Lecture 8, or by using a function.\n",
    "Code for this may be shared and discussed on the forum.\n",
    "\n",
    "(vi) Evaluate formula fn in the four models to obtain four truth values. Compare them\n",
    "to the target truth values.\n",
    "\n",
    "Problems\n",
    "=======\n",
    "\n",
    "1. letter two is consonantal   \n",
    "Base your analysis on 'letter two is vocalic', which is covered by the base grammar.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. every vowel precedes letter three\n",
    "   Add a production for 'precedes' to the grammar. Don't add it to the\n",
    "   valuations. Instead, define the semantics of 'precedes' in terms of\n",
    "   the primitive used for 'follows'.\n",
    "\n",
    "3. some vowel is capitalized\n",
    "   You need to add \"capitalized\" to the grammar, and to the valuations.\n",
    "\n",
    "4. no glide is capitalized\n",
    "  The glides are \"y\" and \"w\". Define \"no\", with the same semantic\n",
    "    type as \"every\" and \"some\". Include a constant 'glide' in the valuations.\n",
    "\n",
    "5. letter one is initial and is a consonant\n",
    "   Define \"initial\", meaning 'is at the start of the word' in terms\n",
    "   of the available primitives.\n",
    "\n",
    "Work individually, except that code for mapping a word to a valuation may be\n",
    "shared.  Post techinical questions and requests for hints on the forum.\n",
    "\n",
    "Notes\n",
    "The problems are selected so that quantifiers are used only in subject position.\n",
    "So it is not necessary (except perhaps in challenge problems) to apply the strategy\n",
    "from simple-sem.fcfg to fit quantified NPs into object positions.\n",
    "\n",
    "The words 'precedes' and 'follows' are interpreted in the sense of 'not necessarily\n",
    "immediately'.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CompLing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6efd08fb7d61c09f4aaea9e8ceb0bf572fefe064d17a4cedc1b735c081f854a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
